{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_transfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EO6YtBnTnl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQIVCGsYTIWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as K\n",
        "from tensorflow import keras as K\n",
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY1rIuQ6Oeot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c5e278d-9cf9-414e-810c-8595efe50129"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKSiS6okT0sA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(X, Y):\n",
        "    \"\"\"\n",
        "    Pre-processes the data for the model\n",
        "\n",
        "        :param X: numpy.ndarray of shape (m, 32, 32, 3)\n",
        "            containing the CIFAR 10 data, where m is the\n",
        "            number of data points\n",
        "\n",
        "        :param Y: numpy.ndarray of shape (m,) containing\n",
        "            the CIFAR 10 labels for X\n",
        "\n",
        "        :returns: X_p, Y_p\n",
        "    \"\"\"\n",
        "    X_p = K.applications.densenet.preprocess_input(X)\n",
        "\n",
        "    # encode to one-hot\n",
        "    Y_p = K.utils.to_categorical(Y, 10)\n",
        "    return X_p, Y_p\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQro04MLp8WC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "27a8c059-2305-4a7a-c93b-8455df8758bd"
      },
      "source": [
        "CALLBACKS = []\n",
        "MODEL_PATH = 'cifar10.h5'\n",
        "optimizer = K.optimizers.Adam()\n",
        "\n",
        "# load cifar 10\n",
        "(x_train, y_train), (x_test, y_test) = K.datasets.cifar10.load_data()\n",
        "\n",
        "# pre-procces data\n",
        "x_train, y_train = preprocess_data(x_train, y_train)\n",
        "x_test, y_test = preprocess_data(x_test, y_test)\n",
        "\n",
        "# input tensor\n",
        "inputs = K.Input(shape=(32, 32, 3))\n",
        "\n",
        "# upscale layer\n",
        "upscale = K.layers.Lambda(lambda x: tf.image.resize_image_with_pad(x,\n",
        "                                                                  160,\n",
        "                                                                  160,\n",
        "                                                                  method=tf.image.ResizeMethod.BILINEAR))(inputs)\n",
        "\n",
        "# load base model\n",
        "base_model = K.applications.DenseNet121(include_top=False,\n",
        "                                        weights='imagenet',\n",
        "                                        input_tensor=upscale,\n",
        "                                        input_shape=(160,160,3),\n",
        "                                        pooling='max')\n",
        "\n",
        "# freeze layers to avoid destroying any of the information they\n",
        "# contain during future training rounds\n",
        "#for i in base_model.layers[:200]:\n",
        "#  i.trainable = False\n",
        "#base_model.trainable = False\n",
        "\n",
        "\n",
        "# add top layers\n",
        "out = base_model.output\n",
        "out = K.layers.Flatten()(out)\n",
        "out = K.layers.BatchNormalization()(out)\n",
        "out = K.layers.Dense(256, activation='relu')(out)\n",
        "out = K.layers.Dropout(0.3)(out)\n",
        "out = K.layers.BatchNormalization()(out)\n",
        "out = K.layers.Dense(128, activation='relu')(out)\n",
        "out = K.layers.Dropout(0.3)(out)\n",
        "out = K.layers.BatchNormalization()(out)\n",
        "out = K.layers.Dense(64, activation='relu')(out)\n",
        "out = K.layers.Dropout(0.3)(out)\n",
        "out = K.layers.Dense(10, activation='softmax')(out)\n",
        "\n",
        "# callbacks\n",
        "CALLBACKS.append(K.callbacks.ModelCheckpoint(filepath=MODEL_PATH,\n",
        "                                              monitor='val_acc',\n",
        "                                              save_best_only=True))\n",
        "\n",
        "CALLBACKS.append(K.callbacks.EarlyStopping(monitor='val_acc',\n",
        "                                            verbose=1,\n",
        "                                            patience=5))\n",
        "\n",
        "CALLBACKS.append(K.callbacks.TensorBoard(log_dir='logs'))\n",
        "\n",
        "# model compile\n",
        "model = K.models.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# train\n",
        "model.fit(x=x_train,\n",
        "          y=y_train,\n",
        "          batch_size=64,\n",
        "          epochs=20,\n",
        "          callbacks=CALLBACKS,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 555s 11ms/sample - loss: 0.8571 - acc: 0.7244 - val_loss: 0.7360 - val_acc: 0.7611\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 350s 7ms/sample - loss: 0.4641 - acc: 0.8582 - val_loss: 0.6721 - val_acc: 0.7838\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 350s 7ms/sample - loss: 0.3635 - acc: 0.8854 - val_loss: 0.4796 - val_acc: 0.8480\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 350s 7ms/sample - loss: 0.3101 - acc: 0.9034 - val_loss: 0.4916 - val_acc: 0.8549\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 350s 7ms/sample - loss: 0.2654 - acc: 0.9174 - val_loss: 0.3012 - val_acc: 0.9033\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 349s 7ms/sample - loss: 0.2362 - acc: 0.9254 - val_loss: 0.3558 - val_acc: 0.8820\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 350s 7ms/sample - loss: 0.2152 - acc: 0.9317 - val_loss: 0.2806 - val_acc: 0.9073\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 348s 7ms/sample - loss: 0.1911 - acc: 0.9404 - val_loss: 0.2994 - val_acc: 0.9048\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 349s 7ms/sample - loss: 0.1728 - acc: 0.9457 - val_loss: 0.2749 - val_acc: 0.9123\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 348s 7ms/sample - loss: 0.1448 - acc: 0.9546 - val_loss: 0.4813 - val_acc: 0.8672\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 348s 7ms/sample - loss: 0.1438 - acc: 0.9550 - val_loss: 0.3512 - val_acc: 0.8965\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 349s 7ms/sample - loss: 0.1230 - acc: 0.9612 - val_loss: 0.2959 - val_acc: 0.9127\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 348s 7ms/sample - loss: 0.1104 - acc: 0.9658 - val_loss: 0.3124 - val_acc: 0.9170\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 346s 7ms/sample - loss: 0.1046 - acc: 0.9676 - val_loss: 0.3533 - val_acc: 0.9065\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 346s 7ms/sample - loss: 0.0926 - acc: 0.9716 - val_loss: 0.3134 - val_acc: 0.9102\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 346s 7ms/sample - loss: 0.0927 - acc: 0.9715 - val_loss: 0.3399 - val_acc: 0.9086\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 346s 7ms/sample - loss: 0.0802 - acc: 0.9746 - val_loss: 0.3546 - val_acc: 0.9064\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 347s 7ms/sample - loss: 0.0803 - acc: 0.9742 - val_loss: 0.2926 - val_acc: 0.9217\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 348s 7ms/sample - loss: 0.0785 - acc: 0.9753 - val_loss: 0.2977 - val_acc: 0.9222\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 348s 7ms/sample - loss: 0.0670 - acc: 0.9790 - val_loss: 0.2714 - val_acc: 0.9283\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f942869b4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SjW13g30OsO",
        "colab_type": "text"
      },
      "source": [
        "Epoch 20/20\n",
        "50000/50000 [==============================] - 348s 7ms/sample - loss: 0.0670 - acc: 0.9790 - val_loss: 0.2714 - val_acc: 0.9283\n",
        "\n",
        "<tensorflow.python.keras.callbacks.History at 0x7f942869b4e0>\n"
      ]
    }
  ]
}